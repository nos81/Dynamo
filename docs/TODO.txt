======================
TODO list for Dynamo 2
======================

(*) = done


1. Sample/reference problem set, unit tests
     (1) Include all the reference problems from Dynamo 1.2
     (2) Random problem generator
     (3) Demos and examples
         (*)  all basic optimization tasks (open systems, _partial tasks...)
         (*)  optimizing taus
         (*)  ensemble optimization
      4. Tests
        (*) test_gradient
         *  unit tests
           *  the same unitary (closed) optimization problem should yield a similar solution in both "closed" and "open" modes
           *  the same unitary sequence must return the same error in both "closed" and "open" modes
        (*)  script that runs all the demos (to make sure they still run!)

2. Minimal benchmarking framework
      1. Allow user to easily compare several algorithms for a given problem
      2. Produce spaghetti plots

3. Robust data provenance, reproducibility
     (1) Store in the dataset all necessary metadata, such as
         Dynamo version, a timestamp, author name, description string etc.
         Going from description to H is easy, H to description hard/impossible.
     (2) Store the optimization parameters, initial sequence etc. so
         the exact same optimization can be re-run any time. Add a
         function for this.
     (3) In the system description, include also dimension vector and
         labels for the states and controls.

4. Controls: transformations, limits, nonlinearity etc.
     (1) Implementation method: "fold" controls through a linearly scaled "sin" function
     (2) taus as optimization parameters
      3. Nondiagonal transformations, i.e. x = A cos(B), y = A sin(B). 
      4. Shai's Jacobian stack?
           * Strange bases for controls: CRAB?
      5. Preconditioning?

5. Additional documentation
     (1) Inline in the code
     (2) At function headers so Matlab's "help" gives more meaningful results
      3. Wiki
      4. Update README.txt
     (5) Add reference to github for those wishing to contribute
      6. dynamo_manual.tex
     (7) Bibliography/references

6. More optimization tasks
      1. Closed S, where we only care about a given subspace
          (*) unitary gates
      2. Closed S+E, where we only care about S
           * (unitary gates), state transfer
      3. Open S+E, Markovian evolution, where we only care about S.
           * unitary gates/general maps, (state transfer)
     (4) Co-operative gates using the projector technique

7. More algorithms
      1. "Integrators"
           * (expm), (eigendecomposition), expv, ode solvers (for time-dependent H:s and H_c:s), t-DMRG, Taylor
      2. Gradients
           * (GRAPE), (eigendecomposition, finite diff, Taylor series (with scaling and squaring), auxiliary matrix)
      3. Optimizers
           * (BFGS native), (BFGS using fminunc), L-BFGS, Newton-Raphson, Monotonic Krotov variant?, NM simplex (no gradient needed)

8. User interface / features
      1. More button-type ui controls for the ui window
          *  Enable/disable tau optimization
          *  save a copy/snapshot
          *  pause
          *  plotting interval
          *  bin splitting
      2. Better API
          *  fix bad/undescriptive names
      3. Other features
         (*)  fix the stats system so that an interrupted optimization run can be continued
          *  automatic top curve sweeping function
          *  plot axis labels should have physical units if possible (qsystem.TU)

9. Cleanup
      1. Delete unused functions
      2. Delete old demos


Random ideas and things to test:

- Demo: target = eye() with relaxation

- When are the controls kind of self-limited? Only if we need to wait
  for H_drift (coupling) to complete the operation?

- express a state transfer problem in a Hermitian basis => real
  vectors and matrices instead of complex ones, faster...

- consistent function interface: dim as first or second parameter?

- Automatic choice of \epsilon for finite-difference gradient method,
  Fouquieres et al., J. Magn. Reson. 212, 412 (2011).

- performance-compare error_full and the approximation given by error_real

- automatic top-curve sweeper function, re-use optimal sequence from neighboring T, re-scale to new T
- compare top-curve-sweeping for determining optimal T with direct tau-optimizing

- Lots of profiling. why do we use almost 30% more time in task1 than Dynamo 1.2? recompute_timeslots is called more?!?
  we seem to need more optimization rounds, maybe sth wrong with the gradient?

- maybe it's not a good idea to normalize the error function with
  |X_f|^2 when doing state transfer since it (being the purity) varies?

- make setting control bounds/params easier/more automated

- make sure that (printed) error function values are (1) somehow physical, (2) same/comparable between all optimization tasks

- problems resulting from using an approximated gradient (incl. finite diff with bad epsilon): breaking symmetries in controls, bad convergence, fake "dynamic" local minima?

- open state overlap mode: warn if final state is not pure

- exact full gradient by auxiliary matrix method: use sparse matrices, or Krylov subspace methods???
  Krylov propagation => aux matrix becomes equivalent to commutator series?

- task: k-state transfers (paper by A Spoerl (PRB, 2011))

- analytic and piecewise analytic controls: http://arxiv.org/abs/1507.04261

- Hessian conditioning also for BFGS, get rid of saddle-point related? shelves/plateaus in convergence

- gpuarray / CUDA, parallelization

- GUI: buttons: stop, resume, plot_pop etc.

- store individual ensemble item errors in cache?
  plot E of slices of ensembles

- save/reload/resume

- update manual, explain important classes and methods

- task: states/unitaries divided into blocks, where phase differences between blocks are unimportant?
  left multiply U with the corresponding phase gate, minimize error over the phases.
  extreme case: unitaries with arbitrary phases (when we only care about populations)
  E(U,T) = 1-1/n \sum_k | <k|U T^\dagger|> |

- task: maximize the expectation value of an operator / make it as close to a target value as possible
