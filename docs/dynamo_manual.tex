\documentclass[aps, pra, a4paper, longbibliography, superscriptaddress]{revtex4-1}

\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage[australian]{babel}
\usepackage{graphicx, hyperref, amsmath, amssymb, verbatim}

\newcommand{\I}{\openone}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\eq}{\Leftrightarrow}

\newcommand{\R}{{\mathbb R}}  % real numbers
\newcommand{\C}{{\mathbb C}}  % complex numbers

\newcommand{\ket}[1]{\left| #1 \right \rangle}
\newcommand{\bra}[1]{\left \langle #1 \right|}
\newcommand{\braket}[2]{\left \langle #1 | #2 \right \rangle}
\newcommand{\ketbra}[2]{\left| #1 \right \rangle \left \langle #2 \right|}
\newcommand{\comm}[2]{\left[ #1, #2 \right]}
\newcommand{\inprod}[2]{\left\langle #1, #2 \right\rangle}

\newcommand{\hilb}[1]{\mathcal{#1}}

\DeclareMathOperator{\End}{End} % endomorphisms
\DeclareMathOperator{\Ad}{Ad}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\cvec}{vec}
\DeclareMathOperator{\diag}{diag}
\newcommand{\vecop}[1]{\widehat{#1}}
\newcommand{\spr}{\Upsilon}
\newcommand{\Xt}{X_T}  % target state
\newcommand{\Xo}{X_0}  % initial state
\newcommand{\Xn}{X_n}  % final (propagated) state

\newcommand{\dd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\wrt}[1]{\:\mathrm{d}#1\:} % (integral) with respect to

% TODO Dynamo vs DYNAMO
\newcommand{\DYNAMO}{\textsc{dynamo}}

\begin{document}
\title{DYNAMO manual}
\date{\today}

\author{Ville Bergholm}
\email{ville.bergholm@iki.fi}
\affiliation{Department of Chemistry, Technische Universität München, Germany}
\affiliation{ISI Foundation, Via Alassio 11/c, 10126 Torino, Italy}

% \pacs{}
% \keywords{tensor network, invariant} not needed as they are embedded into the PDF.  

\begin{abstract}
\DYNAMO{}...
\end{abstract}
\maketitle



\tableofcontents


\section{Introduction}

For \DYNAMO{} benchmarking and mathematical analysis, see~\cite{machnes_2011}.



\section{Basics}

\subsection{Bilinear control system}
% TODO the word system is used for both S alone and the whole thing
The \DYNAMO{} package is designed to handle different types of bilinear
control system optimization problems.
All these problems can be described using a linear master equation of
the following form:
\be
\label{eq:master}
\dot{X}(t) = \underbrace{\left(A(t) +\sum_{c} u_c(t) B_c\right)}_{G(t)} X(t) = G(t) X(t),
\ee
where $u_c$~are scalar \emph{control fields}, $B_c$~the
corresponding \emph{control generators},
$A$~the \emph{drift} generator, $X$~the state of the system (vector or operator),
and $G$~the total generator.
% NOTE We used to have a minus sign in front of~$G$ as a convention, not any more.
Depending on the problem,
$X$~can be related to either an abstract
inner product space, a quantum mechanical Hilbert state space,
or a Liouville space.
We use the $\cvec$-representation of state
operators, explained in Appendix~\ref{sec:vec},
as the mapping between the Hilbert and Liouville spaces.

The master equation can be solved using the propagator ansatz
$X(t) = P(t, t_0) X(t_0)$, with $P(t,t) = \I$, for any initial state~$X(t_0)$,
with the propagator $P(t,t_0)$ following the corresponding operator equation
\be
\label{eq:prop}
\dot{P}(t, t_0) = G(t) P(t, t_0).
\ee


The optimization problems can be divided into three categories:
\begin{itemize}
\item
abstract linear systems
\item
closed quantum systems (unitary evolution generated by a Hamiltonian)
\item
open quantum systems coupled to Markovian baths (Lindblad-form
evolution generated by a Liouvillian)
\end{itemize}
In the quantum cases the system may be divided into the system
proper~$S$ and an an environment~$E$ coupled to it.
The evolution of the compound system $S+E$ is treated
fully quantum mechanically enabling non-Markovian recurrence effects,
but in the end we are only interested in the state of~$S$.



\subsection{Piecewise-defined controls}

% sum over control fields: c
% sum over time slices: k
Assume that the controls are
piecewise constant\footnote{Actually ``piecewise defined'' might be
  enough, if the controls are not constant it's just the computation
  of the propagators~$P_k$ for each slice, and the corresponding derivatives, that gets harder.}
in time: $u_{c, k}$, with $n$~time slices in total,
and that the duration of the $k$th slice is~$\tau_k$.
The points in time where the controls may change in value are thus
\be
t_k = t_0 + \sum_{j=1}^{k} \tau_j, \quad k \in \{0, \ldots, n\}.
\ee
For each time slice we now obtain the propagator
$
P_k := \exp(G_k \tau_k),
$
with
\be
\label{eq:Xn}
X(t_k) = \prod_{j=1}^{k} P_j X(t_0).
\ee
The propagators, like the generators, always act on~$X$ by multiplication
from the left.
Furthermore, let us define the system and adjoint system propagators:
\begin{align}
\spr_k &:= P_k \cdots P_1,\\      % \spr_k X_0 = X_k
\Lambda_k &:= P_n \cdots P_{k+1}.
\end{align}
The relationship between the time slots and the related operators is
illustrated in Table~\ref{table:slices}.


\begin{table}[h]
\[
\begin{array}{c}
\begin{array}{@{t_0}p{1.9em}@{t_1}p{1.8em}@{t_2}p{1.4em}@{t_{k-1}}p{1.1em}@{t_{k}}p{1.1em}@{t_{k+1}}p{0.8em}@{t_{n-2}}p{0.8em}@{t_{n-1}}p{1.3em}@{t_n}}
& & & & & & &
\end{array}\\
\begin{array}{|p{2em}|p{2em}|p{2em}|p{2em}|p{2em}|p{2em}|p{2em}|p{2em}|}
 $\tau_1$ & $\tau_2$ & & $\tau_k$ & $\tau_{k+1}$ & & $\tau_{n-1}$ & $\tau_n$ \\
% $u_{c,1}$ & $u_{c,2}$ & & $u_{c,k}$ & $u_{c,k+1}$ & & $u_{c,n-1}$ & $u_{c,n}$ \\
 $P_1$ & $P_2$ & $\cdots$ & $P_k$ & $P_{k+1}$ & $\cdots$ & $P_{n-1}$ & $P_n$ \\
\cline{1-4}
& & & $\spr_{k}$ & $\Lambda_{k}$ & & & \\
\cline{5-8}
\end{array}
\end{array}
\]
\caption{Time slices and operators related to them.
$t_k = t_0 + \sum_{j=1}^{k} \tau_j$.
The total system and adjoint system
propagators to the point $t_k$ are defined as
$\spr_k = P_k \cdots P_1$ and
$\Lambda_k = P_{n} \cdots P_{k+1}$.
\label{table:slices}
}
\end{table}


\subsection{Propagator gradients}

We may compute the partial derivatives of a slice propagator~$P$ with
respect to the control fields~$u_c$ and the slice duration~$\tau$ by
differentiating the Taylor series for the $\exp$ function term by
term~\cite{Najfeld1995,deFouquieres2011}.
Here we drop the slice index~$k$ since it is evident from the context.
\begin{align}
\dd{P}{\tau}  &= G P = P G,\\
\label{eq:dPdu_series}
\dd{P}{u_{c}}
&=
\sum_{p=1}^{\infty} \frac{\tau^p}{p!}
\sum_{q=0}^{p-1}
G^{q} \dd{G}{u_{c}} G^{p-q-1}
=
\sum_{p=1}^{\infty} \frac{\tau^p}{p!}
\sum_{q=0}^{p-1}
G^{q} B_c G^{p-q-1}
=
\sum_{p=0}^\infty \sum_{q=0}^\infty
\frac{\tau^{p+q+1}}{(p+q+1)!}
G^{p} B_c G^{q}.
\end{align}

Using the beta function integral
\be
\frac{1}{(p+q+1)!} = \frac{B(p+1, q+1)}{p! \: q!} = \frac{1}{p! \: q!} \int_0^1 (1-x)^p x^q \wrt{x},
\ee
and exchanging the relative order of the integration and the summations,
we obtain
\begin{align}
\label{eq:dPdu_int}
\dd{P}{u_{c}}
&=
\int_0^1 \wrt{x}
\sum_{p=0}^\infty 
\frac{((1-x)\tau G)^{p}}{p!}
(\tau B_c)
\sum_{q=0}^\infty
\frac{(x \tau G)^{q}}{q!}
=
\int_0^1 \wrt{x}
e^{(1-x)\tau G}
(\tau B_c)
e^{x \tau G}
=
P
\int_0^1 \wrt{x}
\Ad_{e^{-x\tau G}}
(\tau B_c).
\end{align}

Rewriting the integrand using the $\Ad_{\exp(X)} = \exp(\ad_X)$ trick,
we obtain
\begin{align}
\dd{P}{u_{c}}
&=
P
\int_0^1 \wrt{x}
e^{-x \tau \ad_{G}}
(\tau B_c)
=
\int_0^1 \wrt{x}
e^{x \tau \ad_{G}}
(\tau B_c)
\quad P.
\end{align}
The integral 
$\int_0^1 \wrt{x} e^{x Z}$
evaluates to the power series
$\sum_{k=0}^\infty \frac{Z^k}{(k+1)!}$,
which finally yields the commutator series
\be
\label{eq:dPdu_commseries}
\dd{P}{u_{c}}
= P \left(\tau B_c -\frac{\tau^2}{2!}\comm{G}{B_c}
+\frac{\tau^3}{3!}\comm{G}{\comm{G}{B_c}}
+\ldots \right)
= \left(\tau B_c +\frac{\tau^2}{2!}\comm{G}{B_c}
+\frac{\tau^3}{3!}\comm{G}{\comm{G}{B_c}}
+\ldots \right) P.
\ee

\subsubsection{General case}

We may compute the integral in Eq.~\eqref{eq:dPdu_int}
using the auxiliary matrix technique
\begin{align}
\exp \left(
\begin{pmatrix}
A & B\\
0 & C
\end{pmatrix} \tau\right)
=
\begin{pmatrix}
e^{A \tau} &
e^{A \tau} \int_0^\tau \wrt{x} e^{-A x} B e^{C x}
\\
0 & e^{C \tau}
\end{pmatrix},
\end{align}
which yields
\begin{align}
\exp \left(
\begin{pmatrix}
G & B_c\\
0 & G
\end{pmatrix} \tau\right)
=
\begin{pmatrix}
P & \dd{P}{u_c}\\
0 & P
\end{pmatrix}.
\end{align}
The downside is that computing the matrix expontential is expensive and slow.

\subsubsection{Truncated series}

If $\|\tau G\| \ll 1$, we may approximate the partial derivative by
$\tau P B_c$ (or equivalently by $\tau B_c P$).
This first-order approximation is exact if $\comm{G}{B_c} = 0$.
It is also possible to use a higher-order approximation by including
more terms from the series in Eq.~\eqref{eq:dPdu_commseries}.
The downside is that we must keep track of~$\|G\|$ and adjust the
slice duration~$\tau$ accordingly.

Another option is the scaling and squaring method~\cite{Higham2009}.
First one computes
$n = \lceil \log_2 \|\tau G\|\rceil$,
and sets $s = 2^n$.
This guarantees that
$\|\tau G/s\| \in [0,1]$,
which means that
$P' := e^{\tau G/s}$
can be computed using a quickly converging Taylor series expansion.
Now the propagator can be written in the form
\be
P = \left(e^{\tau G/s}\right)^{s} = P'^{(2^n)},
\ee
i.e. $P'$ squared $n$~times.
The partial derivatives can be computed using the iterated chain rule,
\be
\dd{P}{u_{c}}
= \dd{}{u_{c}} P'^{(2^{n})}
= \left(\dd{}{u_{c}} P'^{(2^{n-1})}\right) P'^{(2^{n-1})} +P'^{(2^{n-1})} \left(\dd{}{u_{c}} P'^{(2^{n-1})}\right),
\ee
all the way down to
$\dd{}{u_{c}} P'$
which is computed using Eq.~\eqref{eq:dPdu_commseries}.


\subsubsection{Normal generators}
\label{sec:dPdu_normal}

In the special case where $G$~is normal, we may insert the spectral decomposition
$G = \sum_m \lambda_m \ket{\lambda_m} \bra{\lambda_m}$
into Eq.~\eqref{eq:dPdu_series}, which yields
\begin{align}
\bra{\lambda_a} \dd{P}{u_{c}} \ket{\lambda_b}
&=
\bra{\lambda_a} \sum_{p=1}^{\infty} \frac{\tau^p}{p!} \sum_{q=0}^{p-1}
G^{q} B_c G^{p-q-1} \ket{\lambda_b}
=
\sum_{p=1}^{\infty} \frac{\tau^p}{p!} \sum_{q=0}^{p-1}
\lambda_a^{q} \lambda_b^{p-q-1} \bra{\lambda_a} B_c \ket{\lambda_b}.
\end{align}
If $\lambda_a = \lambda_b$, we obtain
\begin{align}
\bra{\lambda_a} \dd{P}{u_{c}} \ket{\lambda_b}
&=
\tau \exp(\tau \lambda_a) \bra{\lambda_a} B_c \ket{\lambda_b}.
\end{align}
If $\lambda_a \neq \lambda_b$ at least one of them (say, $\lambda_b$) has to be nonzero,
and we get
\begin{align}
\notag
\bra{\lambda_a} \dd{P}{u_{c}} \ket{\lambda_b}
&=
\sum_{p=1}^{\infty} \frac{\tau^p}{p!} \lambda_b^{p-1}
\sum_{q=0}^{p-1} \left(\frac{\lambda_a}{\lambda_b}\right)^{q}  \bra{\lambda_a} B_c \ket{\lambda_b}
=
\sum_{p=1}^{\infty} \frac{\tau^p}{p!} \lambda_b^{p-1}
\frac{(\lambda_a/\lambda_b)^p -1}{(\lambda_a/\lambda_b) -1}  \bra{\lambda_a} B_c \ket{\lambda_b}\\
&=
\sum_{p=1}^{\infty} \frac{\tau^p}{p!}
\frac{\lambda_a^p -\lambda_b^p}{\lambda_a -\lambda_b}  \bra{\lambda_a} B_c \ket{\lambda_b}
=
\frac{\exp(\tau \lambda_a) -\exp(\tau \lambda_b)}{\lambda_a -\lambda_b}  \bra{\lambda_a} B_c \ket{\lambda_b}.
\end{align}


\subsection{Complexity considerations}

Assume we have $n$~time slices and $c$~controls per slice.
Computing the propagator for slice~$k$ involves
one exponentiation (complexity~$e$ in terms of matrix multiplications (mms)),
or alternatively one lower-norm exponentiation (complexity $e'$), followed by $m$~squarings.
Likewise, the propagator partial derivatives can be computed using a truncated Taylor series
with $q$ terms, or a shorter Taylor series ($q'$ terms) followed by $m$~squarings.

\begin{table}[h]
\begin{tabular}{lc}
computation & C (in mms)\\
\hline
$\spr$, $\Lambda$, error & $n(C(P) +2)+1$\\
gradient & $nc(C(\partial P)+2)$\\
$P$ & $e$\\
$P$, scaling and squaring & $e'+m$\\
$P$, eig & $2+$eig \\
$\partial P$, eig & $4$\\
$\partial P$, finite diff & $e$\\
$\partial P$, series & $2q$\\
$\partial P$, series, ss & $2(q' +m)+1$
\end{tabular}
\end{table}

The amount of memory to store one full
complex IEEE double precision matrix~$M$, dimension $d\times d$, is
\be
\text{mem}(M) = 2 \times 8 \times d^2~\text{bytes} = 16 d^2~\text{bytes}.
\ee
The cache uses this amount of memory multiplied by $(4n + 2)r$ to store $H$, $P$, $U$, and~$L$,
where $r$~is the ensemble size.

\section{Optimization tasks}

Within each type of system we have several possible optimization tasks.
We usually wish to minimize an operator/vector distance~$d(A,B)$ between a
a target state~$\Xt$ and the
initial state $\Xo$ propagated in time to~$t_n$ under a control sequence,
denoted $\Xn$.
% = \left(\prod_{j=1}^{n} P_j\right) \Xo$.
For computational simplicity, the distance is measured using the Frobenius
norm, induced by the Hilbert-Schmidt inner product explained in
Appendix~\ref{sec:hs}:
\be
d^2(A, B) = \|A-B\|^2
= \inprod{A-B}{A-B}
%=\tr((A-B)^\dagger (A-B))
%= \|A\|^2 +\|B\|^2 -\tr(A^\dagger B) -\tr(B^\dagger A)
%= \|A\|^2 +\|B\|^2 -2 \re \tr(A^\dagger B).
= \|A\|^2 +\|B\|^2 -2 \re \inprod{A}{B}.
\ee
Dividing this expression with twice the target norm squared~$\|A\|^2$ (assumed fixed,
known), we obtain the normalized distance measure
\be
\label{eq:df}
D(A,B)
:= \frac{d^2(A, B)}{2\|A\|^2}
= \frac{1}{2}\left(1 +\frac{\|B\|^2}{\|A\|^2}\right) -f(A, B).
\ee
The factor of two is a convenient scale factor, and
\be
f(A, B)
:= \frac{1}{\|A\|^2} \re \inprod{A}{B}
= \frac{1}{\|A\|^2} \re \tr(A^\dagger B)
\ee
is the normalized \emph{fidelity}\footnote{
There is another widely used quantity called fidelity in quantum information science which is different from the present one.}.
If $A$ and $B$ are Hermitian, $\re$ is unnecessary and the fidelity corresponds to an inner product.
If $\|B\|$ is constant (does not depend on the control sequence), we can see from Eq.~\eqref{eq:df} that
the minimum distance corresponds to maximum fidelity.

A concrete example where simply maximizing the fidelity will fail is when comparing mixed states of a qubit.
In this case $D(A,B) = \frac{|\vec{a}-\vec{b}|^2}{2(1+|\vec{a}|^2)}$
whereas $f(A,B) = \frac{1+\vec{a}\cdot\vec{b}}{1+|\vec{a}|^2}$, where
$\vec{a}$ and $\vec{b}$ are Bloch vectors. If $|\vec{a}| < 1$, the
fidelity is maximized by a $\vec{b}$ that points in the same direction
but has unit length, which clearly does not minimize~$D(A,B)$.


Clearly $f(A, A) = 1$.
The triangle inequality $\|A-B\| \le \|A\|+\|B\|$ yields
\be
\left(1 -\frac{\|B\|}{\|A\|} \right)^2 \le 2D(A, B) \le \left(1 +\frac{\|B\|}{\|A\|} \right)^2,
\ee
and the Cauchy-Schwarz inequality $|\inprod{A}{B}| \le \|A\|\|B\|$
yields
$|f(A, B)|
\le \frac{\|B\|}{\|A\|}$.
%\le \frac{1}{2} \left(1 +\frac{\|B\|^2}{\|A\|^2} \right)


We will now derive error functions for various optimization tasks
based on the distance measure defined above.
In what follows, all quantum states are assumed to be normalized to
unity. $\dim \hilb{H}$~is the dimension of the Hilbert space~$\hilb{H}$ of the system.
The results of this section are summarized in Table~\ref{table:tasks}.



\begin{table}
\begin{tabular}{ll|c|c|c|l|l|l|l|l}
system & task & definition & special & space & $\Xo$ & $\Xt$ & $\|X\|^2$ & propagation & error function\\
\hline
abstract
& vector & $\ket{v_0} \to \ket{v_T}$ && V
& $\ket{v_0}$ &$\ket{v_T}$ & $\braket{v}{v}$ &
\eqref{eq:master} & $E_\text{full}$\\
& matrix & $W_0 \to W_T$ && V
& $W_0$ & $W_T$ & $\tr(W^\dagger W)$ &
\eqref{eq:master} & $E_\text{full}$\\
\hline
closed (S)
& ket & $\ket{\psi_0} \to \ket{\psi_T}$ && H
& $\ket{\psi_0}$ & $\ket{\psi_T}$ & $\braket{\psi}{\psi}$ &
\eqref{eq:master} & $E_\text{abs}$\\
& ket & $\ket{\psi_0} \to \ket{\psi_T}$ & phase & H
& $\ket{\psi_0}$ & $\ket{\psi_T}$ & $\braket{\psi}{\psi}$ &
\eqref{eq:master} & $E_\text{full} \to E_\text{real}$\\
%& $\rho_0 \to \rho_T$ & L & $\cvec(\rho)$ & $P(\rho)$ & $E_\text{real}$\\
& state & $\rho_0 \to \rho_T$ && H
& $\rho_0$ & $\rho_T$ & $P(\rho)$ &
\eqref{eq:vonneumann} & $E_\text{full} \to E_\text{real} + C$\\
& state & $\rho_0 \to \ket{\psi_T}$ & overlap & H
& $\rho_0$ & $\ket{\psi_T}$ & $P(\rho)$ &
\eqref{eq:vonneumann} & $E_\text{real}^*$\\
& gate & $U_0 \to U_T$ && H
& $U_0$ & $U_T$ & $d_S$ &
\eqref{eq:master} & $E_\text{abs}$\\
& gate & $U_0 \to U_T$ & phase & H
& $U_0$ & $U_T$ & $d_S$ &
\eqref{eq:master} & $E_\text{full} \to E_\text{real}$\\
\hline
closed (SE)
& state\_partial & $\rho_0 \to \rho_T$ && H
& $\rho_0 \otimes \rho_E$ & $\rho_T$ & $P(\rho)$ &
\eqref{eq:vonneumann} & $E_\text{full}$???\\
& gate\_partial & $U_0 \to U_T$ && H
& $U_0 \otimes \I_E$ & $U_T \otimes \I_E$ & $d_S d_E$ &
\eqref{eq:master} & $E_\text{abs}$\\
\hline
open (S)
& state & $\rho_0 \to \rho_T$ && L & $\cvec(\rho_0)$ & $\cvec(\rho_T)$ & $P(\rho)$ &
\eqref{eq:master} & $E_\text{full}$\\
& state & $\rho_0 \to \ket{\psi_T}$ & overlap & L
& $\cvec(\rho_0)$ & $\cvec(\ketbra{\psi_T}{\psi_T})$ & $P(\rho)$ &
\eqref{eq:master} & $E_\text{real}^*$\\
& gate & $F_0 \to F_T$ && L & $F_0$ & $F_T$ & $\tr(F^\dagger F)$ &
\eqref{eq:master} & $E_\text{full}$\\
\hline
open (SE)
& state\_partial & $\rho_0 \to \rho_T$ && L
& $\cvec(\rho_0 \otimes \rho_E)$ & $\rho_T$ & $P(\rho)$ &
\eqref{eq:master} & $E_\text{full}$\\
& & $U_0 \to U_T$ && L & TODO
\end{tabular}
\caption{Summary of the optimization tasks.
The default error function is $E_\text{full}$, the (scaled) Frobenius error squared.
For some tasks this is not adequate (e.g. if we need to ignore the global phase, or are just interested in an overlap with a ket).
Also, in closed systems $\|X\|$ is constant, so if there is no environment we may replace $E_\text{full}$ with $E_\text{real}$
which is easier to compute.
The $E_\text{abs}$ error functions can be interpreted as Frobenius errors, but not computed using the standard formula.
For the errors marked with asterisk this interpretation is not possible.
\label{table:tasks}
}
\end{table}


\subsection{Abstract linear system}

In the most abstract case we work in an inner product space~$\hilb{V}$.
The generator~$G(t)$ is not necessarily normal, so
the propagator gradient may \emph{not} be computed using the exact
method in Sec.~\ref{sec:dPdu_normal}.
The evolution is not guaranteed to conserve the norm, which is why we must
use the full error function~$D(A,B)$ instead of just the fidelity.


\subsubsection{Vector transfer $\ket{v_0} \to \ket{v_T}$}

In this case $X = \ket{v} \in \hilb{V}$, a vector, with
$\|X\|^2 = \braket{v}{v}$.
The error function is simply the full error
\be
E = D(\Xt, \Xn) = E_\text{full}(\Xt, \Xn).
\ee

\subsubsection{Full propagator $W_0 \to W_T$}

In this task
$X \in \End(\hilb{V})$
is an operator on~$\hilb{V}$, and
we wish to generate a propagator~$W_T$
starting from the initial propagator~$W_0$ (usually~$\I$).
$\|X\|^2 = \tr(X^\dagger X)$ which is not necessarily constant.
The error function is again
\be
E = D(\Xt, \Xn) = E_\text{full}(\Xt, \Xn).
\ee


\subsection{Closed system}

In a closed system, we work within the Hilbert space~$\hilb{H}$.
The generators~$G_k$ are antihermitian (Hamiltonians multiplied by~$-\frac{i}{\hbar}$)
and the propagators~$P_k$ are thus always
unitary. Consequently, both the norm and the purity of a state is conserved,
and we may use the exact spectral decomposition method in
Sec.~\ref{sec:dPdu_normal} to compute the propagator gradient.

As a variant of this setting,
we may have a system~$S$ coherently coupled to an
environment~$E$, but are only interested in~$S$.
The Hilbert space of the total system is
$\hilb{H} = \hilb{H}_S \otimes \hilb{H}_E$.
If one sets $\dim \hilb{H}_E = 1$, all the expressions should coincide with the ones obtained without~$E$.


\subsubsection{Pure state transfer $\ket{\psi_0} \to \ket{\psi_T}$}
\label{sec:closed-pure}

%Maximize state overlap, essentially
In this case $X = \ket{\psi} \in \hilb{H}$, a Hilbert space state vector.
The states are assumed normalized, such that $\|X\|^2 = \braket{\psi}{\psi} = 1$.
Since global phase has no physical significance, our error function is
\be
E = \min_{\phi \in \R} D(\Xt, e^{i \phi} \Xn)
= 1 -\max_{\phi \in \R} \re \left( e^{i \phi} \tr(\Xt^\dagger \Xn)\right)
= 1 -|\tr(\Xt^\dagger \Xn)|
= E_\text{abs}(\Xt, \Xn) \le 1.
\ee

\begin{comment}
This result can also be obtained using the mixed state formula in the
vec representation, with $\rho = \ketbra{\psi}{\psi}$ and thus
$X = \cvec(\rho) = \ket{\psi^*} \otimes \ket{\psi}$.
The fidelity diagram breaks into two pieces and
we obtain
\be
f(A, B)
= (\re) \left|(\tr)(A^\dagger  B) \right|^2.
\ee
with $0 \le f(A, B) \le 1$.
Thus the problem simplifies back into Hilbert space
(albeit with an extra absolute value squared in the expression for the fidelity), and we may equivalently
choose $X = \ket{\psi}$.
\end{comment}


If global phase matters (NOTE: this is unphysical), we instead obtain
\be
E
= D(\Xt, \Xn)
= 1 -\re \tr(\Xt^\dagger \Xn)
= E_\text{real}(\Xt, \Xn) \le 2.
\ee



\subsubsection{Mixed state transfer $\rho_0 \to \rho_T$}
\label{sec:closed-mixed}

To match the form of Eq.~\eqref{eq:master},
the state operators need to be treated as vectors in Liouville space,
$X~=~\cvec(\rho)$.
This approach is fairly inefficient though, since in a closed system the
propagators are always unitary and the full Liouville space is not
needed. Hence we make an exception here, and \emph{do not} follow Eq.~\eqref{eq:master}.
Instead, we define~$Y = \rho$, and remain within the Hilbert space.
This choice gives $\|Y\|^2 = \tr(\rho^2) = P(\rho)$, the purity of~$\rho$.
Eq.~\eqref{eq:master} is replaced by the von Neumann equation
\be
\label{eq:vonneumann}
\dot{Y} = \comm{G(t)}{Y(t)},
\ee
and instead of Eq.~\eqref{eq:Xn} we have
\be
Y(t_k) = P_k \cdots P_1 Y_0 P_1^\dagger \cdots P_k^\dagger.
\ee
The error function is
\be
E
= D(Y_T, Y_n)
= \frac{1}{2}\left(1 +\frac{\|Y_n\|^2}{\|Y_T\|^2}\right) -\frac{1}{\|Y_T\|^2} \re \tr(Y_T^\dagger Y_n).
%= E_\text{real}(Y_T, Y_n) +\frac{1}{2}\left(\frac{\|Y_n\|^2}{\|Y_T\|^2} -1\right).
\ee
Since under unitary propagation purity is preserved we have
$\|Y_n\| = \|Y_0\|$, a constant, and thus the error function is just $E_\text{real}$ with
$f_{\text{max}} = \frac{1}{2}\left(1 +\frac{\|Y_n\|^2}{\|Y_T\|^2}\right)$.

\begin{comment}
Another option would be to use the $\cvec$-representation $X = \cvec(\rho)$.
$\|X\|^2$~is equivalent to the purity of the state:
\be
\|X\|^2
= \|\cvec(\rho)\|^2
= \|\rho\|^2
= \tr(\rho^2)
= P(\rho).
\ee
Unitary propagation conserves purity, hence $\|\Xn\|$~is constant and we may
simply maximize the fidelity
\be
f(\Xt, \Xn)
= \frac{1}{\|\Xt\|} (\re) \tr(\Xt^\dagger \Xn)
\ee
Furthermore, the fidelity is strictly nonnegative since the
state operators are positive:
\be
0 \le f(\Xt, \Xn) \le \sqrt{\frac{P(\rho_0)}{P(\rho_T)}}.
\ee
If either $\rho_T$ or $\rho_0$ is pure,
$\rho = \ketbra{\psi}{\psi}$,
we have $\|\rho\|^2 = \braket{\psi}{\psi}^2 = 1$, and
the diagram simplifies by splitting up.
\end{comment}

In some cases we are only interested in maximizing the projection of
the state on a target pure state~$Y_T = \ketbra{\psi_T}{\psi_T}$,
with $\|Y_T\| = 1$. In this case the natural error function is
\be
E
= 1 -\bra{\psi_T} Y_n \ket{\psi_T}
= 1 -\tr\left(\ketbra{\psi_T}{\psi_T} Y_n \right)
= 1 -\re \tr\left(Y_T^\dagger Y_n\right)
= E_\text{real}(Y_T, Y_n) \le 1,
\ee
identical to the previous error function when $Y_0$~is pure.
FIXME is there any reason to use the overlap error?


\subsubsection{Unitary propagator $\I \to U_T$}
\label{sec:closed-u}
In this task we wish to generate a unitary gate~$U_T$ up to global
phase, starting from the initial gate~$U_0$.
$X$~is a unitary operator on~$\hilb{H}$, and
thus $\|X\|^2 = \tr(X^\dagger X) = \tr(\I) = \dim \hilb{H}$.
Since global phase does not matter, the error function is
\be
E
= \min_{\phi \in \R} D(\Xt, e^{i \phi} \Xn)
= 1 -\max_{\phi \in \R} \frac{1}{\|\Xt\|^2} \re \left( e^{i \phi} \tr(\Xt^\dagger \Xn)\right)
= 1 -\frac{1}{\|\Xt\|^2}|\tr(\Xt^\dagger \Xn)|
= E_\text{abs}(\Xt, \Xn) \le 1.
\ee
\begin{comment}
We can also get rid of phase by explicitly lifting the problem into
Liouville space (see Eq.~\eqref{eq:L-unitary}),
$X = \vecop{V} = V^* \otimes V$,
and then minimize the operator distance~$D(\Xt, \Xn)$.
Using Eq.~\eqref{eq:hat-product}, the norm squared is 
\be
\|X\|^2 = \|\vecop{V}\|^2
= \tr(\vecop{V}^\dagger \vecop{V})
= |\tr(V^\dagger V)|^2
= |\tr(\I)|^2
= (\dim \hilb{H})^2
= \|V\|^4.
\ee
This is constant, so we may maximize the fidelity instead:
\be
f(\Xt, \Xn)
= \frac{1}{\|\Xt\|^2} \re \tr \left(\Xt^\dagger \Xn \right)
%= \frac{1}{N^2} (\re) \left| \tr \left(V_T^\dagger V_n \right) \right|^2
= \frac{1}{\|V_T\|^4} \left| \tr \left(V_T^\dagger V_n \right) \right|^2.
\ee
It clearly obeys $0 \le f(\Xt, \Xn) \le 1$.
Much like in
Sec.~\ref{sec:closed-pure},
the problem simplifies back into Hilbert space, and we may equivalently
choose~$X = V$.
\end{comment}

Typically $U_0 = \I$.
Even when it is not unity, because of the cyclical invariance of the trace, $U_0$~can be always combined into~$U_T$.
If global phase matters (NOTE: unphysical), we may again directly minimize
$D(\Xt, \Xn)$:
\be
E
= D(\Xt, \Xn)
= 1 -\frac{1}{\|\Xt\|^2} \re \tr(\Xt^\dagger \Xn)
= E_\text{real}(\Xt, \Xn) \le 2.
\ee




\subsubsection{Partial state transfer $\rho_0 \to \rho_T$}

In this case we have in addition to the system~$S$ an environment~$E$.
We want to transform the state~$\rho_0$ on $SE$ such that
on $S$ we obtain the reduced state~$\rho_T$.
$Y$ is a state operator on~$\hilb{H}_{SE}$.
%the initial state is assumed to be of the form~$Y_0 = \rho_0 \otimes \rho_E$.
We also define~$Y_T = \rho_T$.
Again, $\|Y\|^2 = P(\rho)$.
Purity of the full $SE$~state~$Y_n$ is conserved under evolution,
but the purity of the reduced state $\tr_E(Y_n)$ on~$S$ is not.
Like in Sec.~\ref{sec:closed-mixed},
the propagation happens using the von Neumann equation.
\be
E
= D(Y_T, \tr_E(Y_n))
%= \frac{1}{2}\left(1 +\frac{|\tr_E(Y_n)|^2}{\|Y_T\|^2)}\right) -\frac{1}{\|Y_T\|^2} \re \tr(Y_T^\dagger \tr_E(Y_n))
= E_\text{full}(Y_T, Y_n).
\ee


\subsubsection{Partial unitary propagator $\I \to U_T$}
\label{sec:partU}

We wish to generate a unitary gate~$U_T$ operating on $S$ up to global
phase, starting from the identity.
We do not care what happens on the environment~$E$.
$X$ is a unitary operator on~$\hilb{H}_{SE}$, and
thus $\|X\|^2 = \dim \hilb{H}_S \dim \hilb{H}_E$.
Define $\Xo = U_0 \otimes U_E$, with $U_E$ unspecified at this point,
and $\Xt = U_T \otimes \I$.
Following~\cite{kosut_2006,floether_2012}, we have
\begin{align}
\notag
E
&= \min_{\Phi \in \text{U}(\hilb{H}_E)} D(U_T \otimes \Phi, \Xn)
= 1 -\max_{\Phi \in \text{U}(\hilb{H}_E)} \frac{1}{\|\Xt\|^2} \re \tr\left((U_T^\dagger \otimes \Phi^\dagger) \Xn\right)\\
&= 1 -\frac{1}{\|\Xt\|^2} \left\|\tr_S(\Xt^\dagger \Xn)\right\|_{\tr}
%= 1 -\frac{1}{\|\Xt\|^2} \left\|g(\Xt, \Xn)\right\|_{\tr}
%= 1 -\frac{1}{\|\Xt\|^2} \tr \sqrt{Q^\dagger Q}
= E_\text{abs}(\Xt, \Xn),
\end{align}
where
$\Phi$ is an arbitrary unitary map on~$\hilb{H}_E$.
Since the trace norm $\|\cdot\|_{\tr}$ is unitarily invariant, the
choice of $U_E$ does not matter and it can be taken to be~$\I_E$.
Typically also $U_0 = \I_S$.
If it is not, it can be cyclically permuted around the partial trace and combined with~$U_T$,
Note that when $\dim \hilb{H}_E = 1$, these expressions reduce to the ones in Sec.~\ref{sec:closed-u}.


\subsubsection{Obtaining a given value for an observable}

$X = \ket{\psi} \in \hilb{H}$.
For simplicity we assume here that the observable in question is a rank-1 projector,
$O = \Xt \Xt^\dagger$. We wish it to attain the expected value of~$a_T$. The obvious error function is now
\begin{align}
E
&=
\left(\tr\left(\Xt \Xt^\dagger \Xn \Xn^\dagger \right) -a_T \right)^2
= \left(\left|\tr(\Xt^\dagger \Xn)\right|^2 -a_T \right)^2
= \left(\left|g(\Xt, \Xn)\right|^2 -a_T \right)^2.
\end{align}
%or $E_{\text{???}}^2(\Xt, \Xn)$ with $f_{\text{max}} = a_T$.
We may also have requirements for several initial states, in which case we simply sum the errors.

Gradient:
\begin{align}
\dd{E}{u}
&=
4 \left(\left|g\right|^2 -a_T \right) \re \left(g^* \dd{g}{u}\right).
\end{align}


\subsubsection{Unitary propagator on a subspace $\I \to U_T$}
\label{sec:closed-u-sub}
In this task we wish to generate a unitary gate~$U_T$ up to global
phase on an $m$-dimensional subspace of the full Hilbert space.
We assume that we have already rotated the Hilbert space generators and~$U_0$ such that
the subspace of interest forms the first~$m$ rows and columns of the propagators.
$X$~is a unitary operator on~$\hilb{H}$, and
thus $\|X\|^2 = \tr(X^\dagger X) = \tr(\I) = \dim \hilb{H}$.
We set $\Xo = U_0 \oplus \I$.
Since global phase does not matter, the error function is
obtained by minimizing over $W \in U(N-m)$
\begin{align}
\notag
E
&= \min_{W, \phi} D(e^{i \phi} (U_T \oplus W), \Xn)
= 1 -\max_{W, \phi} \frac{1}{N} \re \tr \left(e^{-i \phi} (U_T^\dagger \oplus W^\dagger) \Xn\right)\\
\notag
&= 1 -\max_{W} \frac{1}{N} \left|\tr ((U_T^\dagger \oplus W^\dagger) \Xn)\right|
= 1 -\max_{W} \frac{1}{N} \left|\tr (U_T^\dagger {\Xn}^{(1,1)}) +\tr (W^\dagger {\Xn}^{(2,2)})\right|\\
&= 1 -\max_{W'} \frac{1}{N} \left|\tr (U_T^\dagger {\Xn}^{(1,1)}) +\tr (\Sigma W')\right|
= 1 -\frac{1}{N} \left(\left|\tr (U_T^\dagger {\Xn}^{(1,1)})\right| +\left\|{\Xn}^{(2,2)}\right\|_{\tr} \right),
\end{align}
where we used the SVD ${\Xn}^{(2,2)} = U \Sigma V^\dagger$.
To simplify the gradient computation, we may instead use the error function
\begin{align}
E'
&=
1 -\frac{1}{m} \left|\tr (U_T^\dagger {\Xn}^{(1,1)})\right|
= 1 -\frac{1}{\|\Xt\|^2} \left|\tr (\Xt^\dagger \Xn)\right|
= E_\text{abs}(\Xt, \Xn),
\end{align}
with $\Xt = U_T \oplus 0$.
NOTE: $E'$ is not a Frobenius norm error since we do not include $\|{\Xn}^{(2,2)}\|_{\tr}$.


\subsubsection{Unitary propagator ignoring all phases on one side $\I \to U_T$}

Let $\phi(\vec{a}) = \diag(e^{i \vec{a}})$ be an arbitrary diagonal unitary gate.
$X$ is again a unitary operator on $\hilb{H}$.
The error function is
\begin{align}
\notag
E
&=
\min_{\vec{a}} D(\Xt, \phi(\vec{a}) \Xn)
=
1-\max_{\vec{a}} \frac{1}{\|\Xt\|^2} \re \tr\left(\Xt^\dagger \phi(\vec{a}) \Xn\right)
=
1-\max_{\vec{a}} \frac{1}{\|\Xt\|^2} \re \sum_k \bra{k} \Xn \Xt^\dagger \ket{k} e^{i a_k}\\
&=
1-\frac{1}{\|\Xt\|^2} \sum_k \left|\bra{k} \Xn \Xt^\dagger \ket{k}\right|
\end{align}



\subsection{Open system with a Markovian bath}

In this case our system is coupled to a Markovian bath, and thus
we have to work in a Liouville space~$\hilb{L}$,
with $\dim \hilb{L} = (\dim \hilb{H})^2$.
We use the $\cvec$-representation of state
operators, explained in Appendix~\ref{sec:vec},
as the mapping between the Hilbert and Liouville spaces.
The generators~$G_k$ are Liouvillians, expressible in terms of
Hamiltonians and Lindblad operators, and in general no longer normal.
Hence the propagator gradient may \emph{not} be computed using the exact
method in Sec.~\ref{sec:dPdu_normal}.
Again, as in the closed system case, our total system may also consist of
a system~$S$ coherently coupled to an environment~$E$.

An important difference to the closed system case is that a general Markovian propagation
does not necessarily conserve purity and unitarity.
If the generators~$G_k$ are not antihermitian, we may no longer have
$\|\Xn\|^2 = \|\Xo\|^2$. When this is the case
fidelity does not uniquely define the distance and we must use the
full distance measure~$D(\Xt,\Xn)$ instead.

%\be
%\|\Xn\|^2 = \tr\left(\Xo^\dagger \left(\prod_{k=1}^{n} P_k\right)^\dagger \left(\prod_{k=1}^{n} P_k\right) \Xo\right).
%\ee
%If $G_k$ is normal $\eq \quad [G_k, G^\dagger_k] = 0$, we have
%\be
%P_k^\dagger P_k
%= \exp(\tau_k G^\dagger_k) \exp(\tau_k G_k)
%= \exp(\tau_k (G_k^\dagger + G_k)).
%\ee
%If all the generators $G_k$ are antihermitian, this reduces to $\I$ and thus
%$\|\Xn\|^2 = \tr(\Xo^\dagger \Xo) = \|\Xo\|^2$, but usually this is not
%the case.


\subsubsection{State transfer $\rho_0 \to \rho_T$}

$X~=~\cvec(\rho)$, a vector in Liouville space.
Using Eq.~\eqref{eq:vec-isometry}, we find that
$\|X\|^2$~is equivalent to the purity of the state:
\be
\|X\|^2
= \|\cvec(\rho)\|^2
= \|\rho\|^2
= \tr(\rho^2)
= P(\rho).
\ee
The correct error function is
\be
E
= D(\Xt, \Xn)
%= \frac{1}{2}\left(1 +\frac{\|\Xn\|^2}{\|\Xt\|^2}\right) -\frac{1}{\|\Xt\|^2} \re \tr(\Xt^\dagger \Xn)
= E_\text{full}(\Xt, \Xn).
\ee

If we are only interested in maximizing the projection of
the state on a target pure state~$\ketbra{\psi_T}{\psi_T}$, the natural error
function is
\be
E
= 1 -\bra{\psi_T} \rho_n \ket{\psi_T}
= 1 -\tr\left(\ketbra{\psi_T}{\psi_T} \rho_n \right)
= 1 -(\re \tr)\left(\Xt^\dagger \Xn\right)
= E_\text{real}(\Xt, \Xn) \le 1.
\ee
This is cheaper to compute than~$E_\text{full}(\Xt, \Xn)$, but not equivalent to it.


\subsubsection{General quantum map $F_0 \to F_T$}

$X$ is a general quantum map in the vec-representation on Liouville space.
A unitary target map~$U_T$ would be given by
$\Xt = \vecop{U_T}$, which by Eq.~\eqref{eq:hat-product}
yields $\|\Xt\|^2 = (\dim \hilb{H})^2$.
However, the norm $\|\Xn\|$ of the propagated operator
is not necessarily constant. The error function is again
\be
E
= D(\Xt, \Xn)
= E_\text{full}(\Xt, \Xn).
\ee


\subsubsection{Partial state transfer $\rho_0 \to \rho_T$}

We want to transform the state~$\rho_0$ on $SE$ such that
on $S$ we obtain the reduced state~$\rho_T$.
$X = \cvec(\rho)$ is the vec-representation of a state operator on~$\hilb{H}_{SE}$.
Again, $\|X\|^2 = P(\rho)$,
%The initial state is assumed to be of tensor product form.
%We define $\Xo = \cvec(\rho_0 \otimes \rho_E)$,
and we define~$\Xt = \cvec(\rho_T)$.
The error is obtained using a partial trace over~$E$:
\be
E
= D(\Xt, \tr_E(\Xn))
= E_\text{full}(\Xt, \Xn).
\ee


\subsubsection{Partial unitary propagator $\I \to U_T$}

$X$ is a quantum map in the $\cvec$-representation on the Liouville
space corresponding to the Hilbert space~$\hilb{H}_{SE}$.
$\Xt = \vecop{U_T \otimes \Phi}$
where $\Phi$ is any unitary map on~$\hilb{H}_E$, and
$\Xo = \vecop{U_0 \otimes U_E}$ with $U_E$ unspecified at this point.
Due to the unitarity we have $\|\Xt\|^2 = d_S^2 d_E^2$ regardless of~$\Phi$,
but $\|\Xn\|$~is not constant. Hence we must use the full error function.
\begin{align}
\notag
E
&= \min_{\Phi \in \text{U}(\hilb{H}_E)} D(\vecop{U_T \otimes \Phi}, \Xn)
= \frac{1}{2}\left(1 +\frac{\|\Xn\|^2}{\|\Xt\|^2}\right)
-\max_{\Phi \in \text{U}(\hilb{H}_E)} \frac{1}{\|\Xt\|^2} \re \inprod{\vecop{U_T \otimes \Phi}}{\Xn}.
\end{align}
TODO It seems we cannot use the method in Sec.~\ref{sec:partU}
because the SVD yields generic $U(d_E^2)$ unitaries whereas $\vecop{\Phi}$
is of a more limited form and thus cannot always absorb them.

\subsection{Error functions and their gradients}

Here we summarize all the error functions encountered above, and explain how they and their gradients are computed.
The error functions are
\begin{align}
E_\text{real}(A, B) &:= 1 -\frac{1}{\|A\|^2} \re g(A, B),\\
E_\text{abs}(A, B) &:= 1 -\frac{1}{\|A\|^2} \left\|g(A, B)\right\|_{\tr}
= 1 -\frac{1}{\|A\|^2} \tr \sqrt{g^\dagger g}
= 1 -\frac{1}{\|A\|^2} \tr \Sigma,\\
E_\text{full}(A, B) &:= \frac{1}{2}\left(1 +\frac{\|\tr_E(B)\|^2}{\|A\|^2}\right) -\frac{1}{\|A\|^2} \re \tr(A^\dagger \tr_E(B))
\end{align}
where the auxiliary function $g$ is given by
\begin{align}
g(A, B) &:= \tr_S(A^\dagger B).
\end{align}
In the expression for $E_\text{abs}$ we have used the singular value decomposition $g = W \Sigma V^\dagger$.
Note that when $\dim \hilb{H}_E = 1$, $g(A,B)$ is a scalar and the trace norm in $E_\text{abs}$
reduces to a simple absolute value.


All of these functions and their gradients can be efficiently computed
using the $\spr_k$ and $\Lambda_k$
for any $j, k \in \{0, \ldots, n\}$:
NOTE: for mixed-state error functions we need to cyclically permute stuff around the trace, which is why it has to be a full trace!
\begin{align}
g(\Xt, \Xn) &= \tr_S(\Xt^\dagger \Lambda_k \spr_k \Xo)
= \tr_S(L_{k+1} U_{k+1}),\\
g(Y_T, Y_n) &= \tr(\Lambda_k^\dagger Y_T^\dagger \Lambda_k \spr_k Y_0 \spr_k^\dagger)
= \tr(L_{k+1} U_{k+1}),\\
\notag
E_\text{full}(\Xt, \Xn) &= \frac{1}{2}\left(1 +\frac{\|\tr_E(\Lambda_j \spr_j \Xo)\|^2}{\|\Xt\|^2}\right)
-\frac{1}{\|\Xt\|^2} \re \tr(\Xt^\dagger \tr_E(\Lambda_k \spr_k \Xo))\\
&= \frac{1}{2}\left(1 +\frac{\|\tr_E(L_{j+1} U_{j+1})\|^2}{\|\Xt\|^2}\right)
-\frac{1}{\|\Xt\|^2} \re \tr(\Xt^\dagger \tr_E(L_{k+1} U_{k+1})).
\end{align}

\begin{table}
\begin{tabular}{l|c|c|c}
error function & $U_{k+1}$ = $X_k$ & $L_{k+1}$ & g\\
\hline
$E_\text{abs}$, $E_\text{real}$ & $\spr_k \Xo$ & $\Xt^\dagger \Lambda_k$ & $\tr_S(L_k U_k)$\\
$E_\text{mixed}$ & $\spr_k Y_0 \spr_k^\dagger$ & $\Lambda_k^\dagger Y_T^\dagger \Lambda_k$ & $\tr(L_k U_k)$\\
$E_\text{full}$ & $\spr_k \Xo$ & $\Lambda_k$ & $\tr_E(L_k U_k)$
\end{tabular}
\caption{Definitions of the cached objects $U$, $L$ and $g$ used in \DYNAMO{}
  code. $k \in \{0, \ldots, n\}$.}
\label{table:UL}
\end{table}


The form of the expressions leads us to cache slightly different objects with
each error function to speed up the computation, presented in table~\ref{table:UL}.
The gradients ($A$ is always constant) are
\begin{align}
\dd{E_\text{real}(A,B)}{u}
&= -\frac{1}{\|A\|^2} \re \left( \dd{g}{u} \right),\\
\dd{E_\text{abs}(A,B)}{u}
&= -\frac{1}{\|A\|^2} \re \tr \left(\left(\sqrt{g^\dagger g}\right)^{-1} g^\dagger \dd{g}{u}\right)
= -\frac{1}{\|A\|^2} \re \tr \left(V \Sigma^{-1} \Sigma W^\dagger \dd{g}{u}\right),\\
\dd{g(\Xt, \Xn)}{u} &= \tr_S(\Xt^\dagger \Lambda_{k} \left(\dd{P_k}{u}\right) \spr_{k-1} \Xo)
= \tr_S(L_{k+1} \left(\dd{P_k}{u}\right) U_{k}),\\
\notag
\dd{g(Y_T, Y_n)}{u} &= 2 \re \tr\left(\Lambda_k^\dagger Y_T^\dagger \Lambda_k \left(\dd{P_k}{u}\right) \spr_{k-1} Y_0 \spr_{k-1}^\dagger P_k^\dagger \right)
= 2 \re \tr\left(L_{k+1} \left(\dd{P_k}{u}\right) U_{k} P_k^\dagger \right)\\
&= 2 \re \tr\left(L_{k+1} \left(\dd{P_k}{u}\right) P_k^\dagger U_{k+1} \right), \quad \text{and}\\
\notag
\dd{E_\text{full}(A, B)}{u}
&= \frac{1}{\|A\|^2} \re \tr\left((\tr_E(B)-A)^\dagger \dd{\tr_E(B)}{u}\right)
= \frac{1}{\|\Xt\|^2} \re \tr\left((g -\Xt)^\dagger  \tr_E(\Lambda_{k} \left(\dd{P_k}{u}\right) \spr_{k-1} \Xo) \right)\\
&= \frac{1}{\|\Xt\|^2} \re \tr\left((g -\Xt)^\dagger  \tr_E(L_{k+1} \left(\dd{P_k}{u}\right) U_{k}) \right).
\end{align}
Note that when $\dim \hilb{H}_E = 1$, the gradient of $E_\text{abs}$
reduces to
$-\frac{1}{\|A\|^2} \re \left(\frac{g^*}{|g|} \dd{g}{u} \right)$.\\
Note also the $2 \re$ in the gradient expression  
for the special case of mixed state transfer in a closed system.


\subsection{Modified optimization tasks}

\subsubsection{State operator propagation interspersed with arbitrary linear maps}

In the closed and open state operator transfer tasks we may wish to apply additional fixed linear maps~$Q$
on the state at various points during the sequence, for example to optimize for gate co-operativity.
For the gradient computation it seems that the maps have to be applied to the $U$ and~$L$ objects.

We can implement them using additional bins, and remove the extra bin from the gradient mask so that it is never updated.
In the simplest case (full Liouville space) the $P_k$ in the extra bin is replaced by~$Q$. More generally, we may
use a bin-specific callback function that is given the previous $U$ or~$L$ object as input,
and returns the mapped object.

In the open-system case we use the full Liouville space in which the $Q$~maps are just matrices
that act by left multiplication on~$U$, and by right multiplication on~$L$:
\begin{align*}
\tr(L_{k+1} U_{k+1})
&= \tr(L_{k+1} (Q U_{k}))
= \tr((L_{k+1} Q) U_{k})
= \tr(L_{k} U_{k}).
\end{align*}


In the absence of noise the closed-system case is way more efficient but also more complicated because we work in a Hilbert space.
The maps cannot generally be implemented using conjugation (like the unitary propagators).
Instead we can use a Kraus-like decomposition, $Q(\rho) = \sum_j A_j \rho B_j$,
which is general enough for an arbitrary~$Q$:
\begin{align}
U_{k+1} &= Q(U_k) = \sum_j A_j U_k B_j,\\
L_{k} &= Q'(L_{k+1}) = \sum_j B_j L_{k+1} A_j.
\end{align}
This way
\begin{align*}
\tr(L_{k+1} U_{k+1})
&= \tr(L_{k+1} Q(U_k))
= \sum_j \tr(L_{k+1} A_j U_k B_j)
= \sum_j \tr(B_j L_{k+1} A_j U_k)\\
&= \tr(Q'(L_{k+1}) U_k)
%&= \inprod{L_{k+1}^\dagger}{U_{k+1}}
%= \inprod{L_{k+1}^\dagger}{Q(P_k U_k P_k^\dagger)}
%= \sum_j \inprod{L_{k+1}^\dagger}{A_j P_k U_k P_k^\dagger B_j}
%= \sum_j \inprod{\cvec(L_{k+1}^\dagger)}{\cvec(A_j P_k U_k P_k^\dagger B_j)}\\
%&= \sum_j \inprod{\cvec(L_{k+1}^\dagger)}{(B_j^T P_k^* \otimes A_j P_k)\cvec(U_k)}
%= \sum_j \inprod{(B_j^T P_k^* \otimes A_j P_k)^\dagger \cvec(L_{k+1}^\dagger)}{\cvec(U_k)}
%= \sum_j \inprod{(P_k^T B_j^* \otimes P_k^\dagger A_j^\dagger) \cvec(L_{k+1}^\dagger)}{\cvec(U_k)}\\
%&= \sum_j \inprod{\cvec(P_k^\dagger A_j^\dagger L_{k+1}^\dagger B_j^\dagger P_k)}{\cvec(U_k)}
%= \sum_j \inprod{P_k^\dagger A_j^\dagger L_{k+1}^\dagger B_j^\dagger P_k}{U_k}
= \tr(L_{k} U_{k})
\end{align*}
as it should.
A slightly faster and easier way is to vectorize the Hilbert space operators, apply the $Q$~superop, and de-vectorize:
\begin{align*}
\tr(L_{k+1} U_{k+1})
&= \tr(L_{k+1} \cvec^{-1}(Q \cvec(U_{k})))
= \tr(\cvec^{-1}(Q' \cvec(L_{k+1})) U_{k})
= \tr(L_{k} U_{k}).
\end{align*}
The $Q'$ superop is obtained by transposing~$Q$, and conjugating it with SWAPs.

For the gradient we assume for simplicity that $Q(\rho)^\dagger = Q(\rho^\dagger)$ (or that $Q$ is HP???).
This given, all $U$ and~$L$??? will be hermitian, and we obtain the usual gradient formula.







\section{Package contents}

\DYNAMO{} consists of four major classes used to store the data of an
optimization problem, various utility functions, unit tests and examples.

\subsection{Main classes}

The four main classes are 
\begin{itemize}
\item
\emph{dynamo}: main container object, contains instances of the other classes

\item
\emph{qsystem}: describes the bilinear control system (usually a quantum system, hence the name)

\item
\emph{control\_seq}: stores a control sequence along with all the transformations applied on it

\item
\emph{cache}: takes care of the heavy computing (propagators etc.), caches the results
\end{itemize}

\subsubsection{Dynamo class}

The dynamo class is the main container object for a control optimization problem and possibly its solution.
It contains an instance of the qsystem, control\_seq and cache classes, plus some other data.
\begin{itemize}
\item
\emph{dynamo}: constructor
\item
\emph{version}: returns the version string
\item
\emph{cache\_init}: initializes the cache object
\item
\emph{seq\_init}: initializes the control\_seq object
\item
\emph{full\_mask}: returns a full control mask with or without bin durations
\item
\emph{error}: evaluates the error function and possibly its gradient
at the current point in the optimization space
\item
\emph{update\_controls}: updates the values controls selected in the
mask, marks the cache objects that depend on them stale
\item
\emph{cache\_refresh}: performs all queued computations in the cache
\item
\emph{cache\_fill}: invalidates the cache, recalculates everything
\item
\emph{X}: computes the state of the controlled system at a given time
\item
\emph{plot\_seq}: plots the control sequence
\item
\emph{plot\_stats}: plots the optimization statistics
\item
\emph{plot\_X}: plots the evolution of the system state as a function
of time. Only works for kets and state operators.
\item
\emph{analyze}: plots a report of the optimization run
\item
\emph{easy\_control}: quickly generates a reasonable set of initial
controls, random or not
\item
\emph{init\_opt}: initializes the optimization parameters and statistics
\item
\emph{search\_BFGS}: starts a BFGS optimization run with a given control mask
\item
\emph{ui\_open}: opens a UI window for monitoring the optimization progress
\item
\emph{monitor\_func}: callback for the optimization function, executed
once every iteration. Updates the UI window, decides whether the
optimization should stop here.
\item
\emph{shake}: slightly perturbs the current control sequence. Can be
used for escaping from local minima.
\item
\emph{import}: imports an array containing a control sequence (in
explicit physical units) into the current sequence
\item
\emph{export}: exports the current control sequence into an array with
explicit physical units
\end{itemize}


\subsubsection{Qsystem class}

The qsystem class contains the physical and/or mathematical
description of the bilinear control system we are trying to optimize.

\begin{itemize}
\item
\emph{qsystem}: constructor
\item
\emph{abstract\_representation}:
initializes an abstract bilinear control system
\item
\emph{hilbert\_representation}:
initializes a Hilbert space control system
\item
\emph{vec\_representation}:
initializes a Liouville space control system (in
the vec representation)
\item
\emph{set\_TU}:
sets the time unit, in seconds.
$\hat{A} = A \text{TU}$, $\hat{t} = t / \text{TU}$.
\item
\emph{set\_labels}:
defines some additional data about the system for the benefit of the
human user: description string, state and control labels.
\item
\emph{n\_ensemble}:
returns the number of systems in the ensemble sample
\end{itemize}

\subsubsection{Control sequence class}

The control\_seq class encapsulates a control sequence and the
transformation stack used to transform it.

\begin{itemize}
\item
\emph{control\_seq}: constructor
\item
\emph{n\_timeslots}:
returns the number of timeslots
\item
\emph{n\_controls}:
returns the number of control fields
\item
\emph{get}:
returns the current raw control fields corresponding to the given control mask
\item
\emph{set}:
sets the current raw control fields to the given values, transforms
them, computes the associated derivatives
\item
\emph{inv\_transform}:
performs the inverse control transform on the given set of control
field values, returns the resulting raw control values
\item
\emph{split}:
refines the sequence by splitting the given bins into several pieces
of equal duration
\item
\emph{integral}:
computes the time integrals of the control fields
\item
\emph{plot}:
plots the control field values
\end{itemize}

\subsubsection{Cache class}

The cache class computes and stores the generators and various propagators computed
for each time slice based on the data stored in qsystem and control\_seq.

\begin{itemize}
\item
\emph{cache}: constructor
\item
\emph{invalidate}:
invalidates the entire cache, forces a full recomputation of everything
\item
\emph{mark\_as\_stale}:
forces the recomputation of selected time slices
\item
\emph{refresh}:
recomputes all the cache objects that are both stale and needed
\item
\emph{g\_setup\_recalc}:
determines the optimal way to compute the $g$ object
\end{itemize}


\subsection{Unit tests}

\subsubsection{Test suite}
\emph{test\_suite.m} implements all the test optimization problems used in~\cite{machnes_2011}.
Given the number of the problem as input, it initializes a \DYNAMO{}
instance with the physics and optimization parameters of that particular problem.

\subsubsection{Gradient test}
Given a \DYNAMO{} instance, \emph{test\_gradient.m} tests whether a
given gradient function yields an accurate linearization of the
corresponding error function around the current point in the
optimization space.


\subsection{Utility functions}
In the \emph{utils} directory one can find the following utility functions:
\begin{itemize}
\item
\emph{}:
\end{itemize}

\subsection{Examples}
In the \emph{examples} directory we provide a number of examples of
how to use \DYNAMO{} to solve various optimization problems, to
serve as a tutorial for new users as well as to showcase the most important features.



\appendix
\section{Hilbert-Schmidt inner product}
\label{sec:hs}

We use the Hilbert-Schmidt inner product for both vectors and matrices:
\be
\inprod{X}{Y} := \tr\left(X^\dagger Y\right).
\ee
It induces the Frobenius norm:
\be
\|X\| := \sqrt{\inprod{X}{X}} = \sqrt{\tr\left(X^\dagger X\right)}.
\ee

If $X$ and $Y$ are Hermitian, $\inprod{X}{Y} \in \R$.

\section{$\cvec$ mapping}
\label{sec:vec}

The $\cvec$ function maps Hilbert space operators (square matrices) to
Liouville space vectors by stacking the columns of the matrix in order
from left to right into a column vector. This mapping is clearly
invertible, and we also have
\be
\cvec(A \rho B) = (B^T \otimes A) \cvec(\rho)
\ee
for any Hilbert space operators~$A, B$.
Consequently, the Liouville space equivalent~$\vecop{U}$ for a unitary Hilbert space
propagator~$U$ is
\be
\label{eq:L-unitary}
\vecop{U} := U^* \otimes U,
\ee
since
\be
\cvec(U \rho U^\dagger) = (U^* \otimes U) \cvec(\rho) = \vecop{U} \cvec(\rho).
\ee
We then have for any~$A$,~$B$
\be
\label{eq:hat-product}
\inprod{\vecop{A}}{\vecop{B}}
= \tr((A^* \otimes A)^\dagger (B^* \otimes B))
= \tr((A^T B^*) \otimes (A^\dagger B))
%= \tr((A^\dagger B)^* \otimes (A^\dagger B))
= (\tr(A^\dagger B))^* \: \tr(A^\dagger B)
= |\inprod{A}{B}|^2.
\ee

The following property is also easy to verify:
\be
\label{eq:vec-isometry}
\inprod{\cvec(\rho)}{\cvec(\sigma)} = \cvec(\rho)^\dagger \cvec(\sigma)
= \tr(\rho^\dagger \sigma) = \inprod{\rho}{\sigma}.
\ee





\bibliography{dynamo}
\end{document}
